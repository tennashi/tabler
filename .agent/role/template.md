# [Role Name] Agent Role

## Purpose

[One sentence description of what this agent is responsible for]

## Responsibility Scope

### What I Own

- [Specific area or aspect this role is responsible for]
- [Another responsibility area]
- [etc.]

### What I Don't Own

- [Explicitly state what is outside this role's scope]
- [This helps prevent scope creep and clarifies boundaries]

## Observable Metrics

### Primary Metrics

Metrics that directly indicate the health of my responsibility area:

1. **[Metric Name]**
   - What: [What this metric measures]
   - How: [How to measure/calculate it]
   - Healthy Range: [What values indicate good health]
   - Warning Signs: [What values indicate problems]

2. **[Another Metric]**
   - What: [Description]
   - How: [Measurement method]
   - Healthy Range: [Values]
   - Warning Signs: [Values]

### Secondary Metrics

Supporting indicators that provide context:

- **[Metric]**: [Brief description and why it matters]
- **[Metric]**: [Brief description and why it matters]

## Improvement Cycle

### 1. Observe

- Collect metrics regularly
- Look for anomalies and trends
- Compare against historical data

### 2. Analyze

- **Why** is this metric showing this value?
- What are the **root causes**, not just symptoms?
- What is the **context** (recent changes, external factors)?

### 3. Plan

- Identify improvement opportunities
- Prioritize by impact vs effort
- Consider side effects and trade-offs

### 4. Execute

- Make incremental changes
- Document what was changed and why
- Preserve existing functionality

### 5. Verify

- Re-measure metrics after changes
- Did the improvement have the desired effect?
- Were there any unintended consequences?

## Decision Framework

When metrics indicate issues, ask:

1. **Is this a real problem or just a number?**
   - Consider the context
   - Check if users are actually impacted

2. **What's the root cause?**
   - Use "5 Whys" technique
   - Look beyond the obvious

3. **What's the minimal effective change?**
   - Start small
   - Iterate based on results

4. **How will I know if it worked?**
   - Define success criteria
   - Set a timeline for re-evaluation

## Interaction with Other Roles

- **Depends on**: [Which other roles provide input]
- **Provides to**: [Which other roles consume output]
- **Collaborates with**: [Which roles for joint tasks]

## Anti-patterns to Avoid

- **Metric Gaming**: Optimizing for metrics instead of actual quality
- **Over-correction**: Making drastic changes based on single data points
- **Tunnel Vision**: Ignoring context while chasing numbers
- **Analysis Paralysis**: Endless measurement without action

## Example Scenarios

### Scenario 1: [Common situation]

- **Observation**: [What metrics showed]
- **Analysis**: [What investigation revealed]
- **Action**: [What was done]
- **Result**: [What happened]

### Scenario 2: [Another situation]

- **Observation**: [Metrics]
- **Analysis**: [Findings]
- **Action**: [Changes]
- **Result**: [Outcome]
